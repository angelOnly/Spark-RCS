## 5.2 Kafka 实时数据加工 - KafkaProducer

[kafka producer API](https://kafka.apache.org/21/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)

实现 Kafka producer 发送数据功能。

1. 获取数据集，这里采用验证集中数据进行发送
2. 设置 kafka 配置信息( 根据官网 API 进行配置)
3. 将 testData 转成 DataFrame
4. 迭代消息，利用 KafkaProducer 发送消息
5. 关闭 producer

````scala
object KafkaProducer extends AppConf {
  def main(args: Array[String]) {
    //验证集合数据作为 kafka 的producer，验证集的数据时间更接近现在一点
    val testDF = hc.sql("select * from testData")
    
    //kafka 的配置设置
    val prop = new Properties()
    //设置 broker
    prop.put("bootstrap.servers", "master:9092")
    //设置序列化
    prop.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    prop.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

    prop.put("acks", "all")
    prop.put("retries", "0")
    prop.put("batch.size", "16384")
    prop.put("linger.ms", "1")
    prop.put("buffer.memory", "33554432")

    val topic = "test"
    val testData = testDF.map(x => (topic, x.getInt(0).toString() + "|" + x.getInt(1).toString + "|" + x.getDouble(2).toString()))
    val producer = new KafkaProducer[String, String](prop)
    val messages = testData.toLocalIterator

    while (messages.hasNext) {
      val message = messages.next()
      //对消息封装成 ProducerRecord 对象                          key         value
      val record = new ProducerRecord[String, String](topic, message._1, message._2)
      println(record)
      //发送消息
      producer.send(record)
      Thread.sleep(1000)
    }
    producer.close()
  }
}

````

#### 在 spark-shell 中运行结果

````scala
ProducerRecord(topic=test,partition=null,key=test,value=93439|72294|4.0,timestamp=null)
ProducerRecord(topic=test,partition=null,key=test,value=93622|100487|3.5,timestamp=null)
ProducerRecord(topic=test,partition=null,key=test,value=93439|72294|4.0,timestamp=null)
````

### 问题 1

在发送消息过程中，尝试过使用 testData.foreach 和 testData.map 的方式进行，虽然能正常运行，但是结果产生乱序

**原因**

foreach 和 map 这两种方式会让数据做分布式计算，在计算时候，处理数据是无序的。

项目中 testData 是按照 timestamp 排序的，testData 在 timestamp 上是有序的，所以没有采用 foreach 和 map 的方式。

**解决**

采用 **for**(会序列化问题，见问题 2) 或者 **while**



#### 问题 2

````scala
for (message <- testData) {
    val record = new ProducerRecord[String, String]("test", message._1, message._2)
    println(record)
    producer.send(record)
    Thread.sleep(1000)
}
````

使用 for 循环进行迭代发送数据，报错 java.io.NotSerializableException，KafkaProducer 无法正常序列化

**解决**

采用迭代器进行循环实现

````scala
while (messages.hasNext) {
    val message = messages.next()
    //对消息封装成 ProducerRecord 对象                          key         value
    val record = new ProducerRecord[String, String](topic, message._1, message._2)
    println(record)
    //发送消息
    producer.send(record)
    Thread.sleep(1000)
}
````





