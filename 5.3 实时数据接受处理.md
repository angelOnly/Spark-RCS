## 5.3 实时数据接受处理
````scala
public static JavaPairInputDStream<K,V> createDirectStream(JavaStreamingContext jssc,                                                                                                                                           Class<K> keyClass,                                                                                                                                       Class<V> valueClass,                                                                                                                                            Class<KD> keyDecoderClass,                                                                                                                           Class<VD> valueDecoderClass,                                                                                                                       java.util.Map<String,String> kafkaParams,                                                                                                                                              java.util.Set<String> topics)

Create an input stream that directly pulls messages from Kafka Brokers without using any receiver. This stream can guarantee that each message from Kafka is included in transformations exactly once.
创建一个输入流，直接从Kafka Brokers中提取消息，而无需使用任何接收器。此流可以保证来自Kafka的每条消息都只包含在转换中一次。
````

### 参数

**jssc** - JavaStreamingContext object
**keyClass** - Class of the keys in the Kafka records
**valueClass** - Class of the values in the Kafka records
**keyDecoderClass** - Class of the key decoder
**valueDecoderClass** - Class type of the value decoder
**kafkaParams** - Kafka configuration parameters. Requires "metadata.broker.list" or "bootstrap.servers" to be set with Kafka broker(s) (NOT zookeeper servers), specified in host1:port1,host2:port2 form. If not starting from a checkpoint, "auto.offset.reset" may be set to "largest" or "smallest" to determine where the stream starts (defaults to "largest")
**topics** - Names of the topics to consume
**Returns:**
DStream of (Kafka message key, Kafka message value)

### 实时推荐流程

* web 那边要实时的收集用户登录的动作，从登录动作中封装 kafka 的 producer，然后将 producer 将一系列消息发送出去。
* SparkStreaming 这边再接收用户数据消息，为用户实时产生推荐结果。
* 将推荐结果以一定的方式返回（存入数据库？或者利用 kafka 的 producer 发送推荐结果出去？或写入其它地方，尽快将推荐结果让用户看到）

### 实时推荐实现

1. 每过 5 秒从实时流中取出一条消息
2. 通过该消息的 rdd 中拿到这个 user rdd
3. 获取到 producer 那边发送过来的 recorder，通过 recorder 可以获取到 userid
4. 通过 userid 可以从训练好的模型中获取到推荐结果（通过对 userid 进行判断，老用户按照推荐用户列表推荐，新用户推荐评分最高的）
5. 推荐结果有多种存储方式
   - 写入数据库
   - 在 web 端实时显示
   - 发送到另外的 kafka producer 发送出去

````scala
package xiaoli.com.streaming

import kafka.serializer.StringDecoder
import org.apache.spark.SparkConf
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Duration, StreamingContext}

object SparkDirectStream {
  def main(args: Array[String]) {
    //Duration对象中封装了时间的一个对象，它的单位是ms.
    val conf = new SparkConf().setAppName("SparkDirectStream").setMaster("spark://master:7077")
    //5秒钟接收一条数据
    val batchDuration = new Duration(5000)
    val ssc = new StreamingContext(conf, batchDuration)
    val hc = new HiveContext(ssc.sparkContext)
    val validusers = hc.sql("select * from trainingData")
    val userlist = validusers.select("userId")

    val modelpath = "/tmp/bestmodel/0.8215454233270015"
    val broker = "master:9092"
    val topics = "test".split(",").toSet
    val kafkaParams = Map("bootstrap.servers" -> "master:9092")

    //指定 key，value 的类型以及 key 和 value 的编码方式
    val kafkaDirectStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)

      val messages = kafkaDirectStream.foreachRDD { rdd =>
      //对一个partition来说，只需要加载一次模型即可
      val model = MatrixFactorizationModel.load(ssc.sparkContext, modelpath)
      //取出该partition中是所有users
      val userrdd = rdd.map(x => x._2.split("|")).map(x => x(1)).map(_.toInt)
      def exist(u: Int): Boolean = {
          val userlist = hc.sql("select distinct(userid) from trainingdata").rdd.map(x => x.getInt(0)).toArray()
          userlist.contains(u)
      }
      val validusers = userrdd.filter(user => exist(user))
      val newusers = userrdd.filter(user => !exist(user))
      val validusersIter = validusers.toLocalIterator
      val newusersIter = newusers.toLocalIterator
      /**
        * 老用户推荐
        */
      while (validusersIter.hasNext) {
        val recresult = model.recommendProducts(validusersIter.next, 5)
        println("below movies are recommended for you :")
        println(recresult)
      }
      /**
        * 新用户推荐
        */
      val defaultrecresult = hc.sql("select * from top5DF").rdd.toLocalIterator
      while (newusersIter.hasNext) {
        println("below movies are recommended for you :")
        for (i <- defaultrecresult) {
          println(i.getString(0))
        }
      }
    }
    ssc.start()
    ssc.awaitTermination()
  }
}

````



### 问题 1

这种方式实现有 2 个问题

1. model 在每个用户进行推荐的时候都要加载一次，虽然这个项目中内存不大，但是频繁加载依然很消耗内存
2. foreach 循环导致对象不能正常序列化

````scala
val messages = kafkaDirectStream.foreachRDD { rdd =>
    val userrdd = rdd.map(x => x._2.split("|")).map(x => x(1)).map(_.toInt)
          rdd.foreach { record =>
            val user = record._2.split("|").apply(1).toInt
            //每次都加载模型，会占用大量内存，虽然模型不大，但是也不能每次都加载
            val model = MatrixFactorizationModel.load(sc, modelpath)
            val recresult = model.recommendProducts(user, 5)
            println(recresult)
          }
        }
}
````

**解决**

可以采用迭代器的方式来避开对象不能序列化的问题。

````scala
val messages = kafkaDirectStream.foreachRDD { rdd =>
      val model = MatrixFactorizationModel.load(ssc.sparkContext, modelpath)
      val userrdd = rdd.map(x => x._2.split("|")).map(x => x(1)).map(_.toInt)
      val users = rdd.map(x => x._2.split("|")).apply(1).toInt
      val usersIter = users.toLocalIterator
      while (usersIter.hasNext) {
        val recresult = model.recommendProducts(usersIter.next, 5)
        println("below movies are recommended for you :")
        println(recresult)
      }
    }
````

### 问题 2

从 sparkStreaming 获取到 user 是 testData 中拿到的数据，而我们要要根据这个 userid 在model（traningData） 中拿到推荐结果，不能保证所有 userid 都在 traningData 中出现。

即有新用户存在，如果是新用户，可能不能拿到推荐结果。需要对 userid 进行过滤，判断是新用户还是老用户。

**解决**

````scala
def exist(u: Int): Boolean = {
    val userlist = hc.sql("select distinct(userid) from trainingdata").rdd.map(x => x.getInt(0)).toArray()
    userlist.contains(u)
}
val validusers = userrdd.filter(user => exist(user))
val newusers = userrdd.filter(user => !exist(user))
val validusersIter = validusers.toLocalIterator
val newusersIter = newusers.toLocalIterator
````